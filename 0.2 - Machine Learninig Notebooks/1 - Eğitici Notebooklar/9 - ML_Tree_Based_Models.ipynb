{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees and Tree Based Models\n",
    "\n",
    "Decision trees are a general class of machine learning models that are used for both classification and regression. They have even been adapted for the use of outlier detection. The trained models resemble a tree, complete with branches and nodes. The model is essentially a series of questions with yes or no answers, where the resulting tree structure contains all the combination of responses.\n",
    "\n",
    "Tree based models are popular because they mimic human decision making process, work well for a large class of problems, naturally handle multiclassification, and handle a mix of categorical and numerical data. They are also easy to understand and explain. The transparency of a model is often called its *explicability*. Models with low explicability are often referred to as \"black boxes\" and are difficult to derive insight over the process they are modeling.\n",
    "\n",
    "In this notebook, we will discuss the decision tree model and understand how they are trained and used to make predictions. Further, we will analyze several advanced machine learning models that use decision trees as a building block to create models with greater predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training decision tree classifiers\n",
    "\n",
    "The best way to understand a decision tree is to construct one and visualize it. We'll train a decision tree classifier on the iris data set and visualize the tree with the `Graphviz` package. The iris data set is a famous data set of 150 observations of three different iris species: setosa, versicolor, and virginica. Each observation has measurements of the petal length and width and sepal length and width, for a total of four features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "# load data set\n",
    "data = load_iris()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "# train decision tree\n",
    "tree = DecisionTreeClassifier(max_depth=3,)\n",
    "tree.fit(X, y)\n",
    "\n",
    "# visual tree\n",
    "graphviz.Source(export_graphviz(tree, \n",
    "                                out_file=None,\n",
    "                                feature_names=data['feature_names'],\n",
    "                                class_names=data['target_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the model resembles an upside down tree and each box represents a node in the tree. Printed in each box is\n",
    "\n",
    "* **samples**: the number of observations in the node.\n",
    "* **Gini**: a measure of node purity.\n",
    "* **value**: the distribution of observations in each class.\n",
    "* **class**: the most common label in the node.\n",
    "\n",
    "At the top of the tree is the __root node__. This node is _split_ to form two branches. Observations that satisfy the criterion printed at the top of the box are moved to one branch while the rest to the other. You can view a decision tree as a model that is making partitions in a space that contains your training data. For classification, the partitions are chosen to separate the different classes while in regression, the partitions are picked to reduce the variance of sample labels. For the tree displayed above, node splits were chosen to lead to an overall reduction of the Gini metric, discussed further in the next section. The nodes that do not branch off are called __terminal nodes__ or __leaves__.\n",
    "\n",
    "With a trained tree, predictions are made on an observation by starting at the root and following the path as a result of the criterion in each node. Once at a leaf, the predicted class is the class with the plurality. For example, if an observation has a petal length of 2.5 cm and a petal width of 1 cm, it will reside in the left most leaf in the figure. Since there were 47 training observations of class versicolor and 1 of virginica that resided in the leaf, any observation landing in this leaf will be predicted as versicolor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our trained tree model only makes splits using two features, the petal length and width, making it easy to visualize our model. The interactive figure below lets you control the splitting process described above for the iris data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "def iris_tree(depth=1):\n",
    "    plt.scatter(X[:, 2], X[:, 3], c=y, cmap=plt.cm.viridis)\n",
    "    \n",
    "    if depth >= 1:\n",
    "        plt.hlines(0.8, 0.8, 7, linewidth=2)\n",
    "    if depth >= 2:\n",
    "        plt.hlines(1.75, 0.8, 7, linewidth=2)\n",
    "    if depth >= 3:\n",
    "        plt.vlines(4.85, 1.75, 2.6, linewidth=2)\n",
    "        plt.vlines(4.95, 0.8, 1.74, linewidth=2)\n",
    "\n",
    "    plt.xlabel('Petal Length (cm)')\n",
    "    plt.ylabel('Petal Width (cm)')\n",
    "    plt.xlim([0.8, 7])\n",
    "    plt.ylim([0, 2.6])\n",
    "    \n",
    "depth_slider = IntSlider(value=0, min=0, max=3, step=1, description='depth')\n",
    "interact(iris_tree, depth=depth_slider);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini impurity\n",
    "\n",
    "For classification, decision trees use the Gini impurity metric, a measure of node impurity. The Gini impurity is equal to the probability of misclassifying an observation if it were randomly labeled based on the distribution of labels of the node. The decision tree will perform node splits that result in reducing the Gini metric. The equation for the Gini impurity for node $m$ is\n",
    "\n",
    "$$ G_m = \\sum_k p_{mk} (1 - p_{mk}), $$\n",
    "\n",
    "where $p_{mk}$ is the fraction of observations of class $k$ in node $m$. Consider two cases where a node has 10 observations belonging to two classes. In the first case, each class has equal representation in the node, [5, 5]. In the second, only the first class is present, [10, 0]. The Gini impurity for both cases are\n",
    "\n",
    "* Case 1: [5, 5]\n",
    "$$ G = \\frac{5}{10} \\left(1 - \\frac{5}{10}\\right) + \\frac{5}{10} \\left(1 - \\frac{5}{10}\\right) = 0.5 $$\n",
    "* Case 2: [10, 0]\n",
    "$$ G = \\frac{10}{10} \\left(1 - \\frac{10}{10}\\right) + \\frac{0}{10} \\left(1 - \\frac{0}{10}\\right) = 0 $$\n",
    "\n",
    "The greater the node purity, the lower the Gini metric. See the plot below of how Gini varies with $p_{mk}$ when there are two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.linspace(1E-6, 1-1E-6, 100)\n",
    "gini = p*(1-p) + (1-p)*p\n",
    "\n",
    "plt.plot(p, gini)\n",
    "plt.xlabel('$p$')\n",
    "plt.ylabel('Gini');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "In chemistry, entropy is a measure of the number of microstates of a system and can be viewed as the amount of disorder. The concept of entropy is useful in other fields such as information theory where entropy is a measure of the amount of missing information. The more information is missing, the more uncertainty exists in a process. The equation for entropy of node $m$ is\n",
    "\n",
    "$$ H_m = -\\sum_{k} p_{mk} \\log_2(p_{mk}).$$\n",
    "\n",
    "Using the same two cases as before when calculating the Gini metric, the entropy is equal to\n",
    "\n",
    "* Case 1: [5, 5]\n",
    "$$ H = -\\left[\\frac{5}{10} \\log_2 \\left(\\frac{5}{10}\\right) + \\frac{5}{10} \\log_2 \\left(\\frac{5}{10}\\right)\\right] = 1  $$\n",
    "* Case 2: [10, 0]\n",
    "$$ H = -\\left[\\frac{10}{10} \\log_2 \\left(\\frac{10}{10}\\right) + \\frac{0}{10} \\log_2 \\left(\\frac{0}{10}\\right)\\right] = 0 $$\n",
    "\n",
    "Similar to the Gini impurity, a more pure node will have lower entropy. Since entropy and Gini impurity are very similar metrics, using either will not make any substantial difference in your classifier. By default, the `DecisionTreeClassifier` class uses the Gini metric but can be switched to entropy by setting `criterion='entropy'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing decision trees for regression\n",
    "\n",
    "The process of constructing a decision tree for regression is the nearly identical as classification. However, instead performing node splits that result in an overall drop of entropy or Gini impurity, splits are chosen to produce nodes with an overall reduction in variance of the training labels. Mathematically,\n",
    "\n",
    "$$ \\Delta V = \\left(\\frac{n_L}{n_L + n_R}\\right)\\sigma^2_L + \\left(\\frac{n_R}{n_L + n_R}\\right)\\sigma^2_R,$$\n",
    "\n",
    "where $n$ and $\\sigma^2$ is the number of nodes and variance in the left $L$ and right $R$ nodes from the split, respectively. For making predictions, once an appropriate leaf has been identified for an observation, the predicted label value is the mean of all training samples that resided in the leaf. Below, we illustrate the trained tree with the California housing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# load data set\n",
    "data = fetch_california_housing()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "# train decision tree\n",
    "tree = DecisionTreeRegressor(max_depth=3)\n",
    "tree.fit(X, y)\n",
    "\n",
    "# visual tree\n",
    "graphviz.Source(export_graphviz(tree, \n",
    "                                out_file=None,\n",
    "                                feature_names=data['feature_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box have entries of \"mse\" and \"value\" with the following meanings:\n",
    "\n",
    "* **mse**: mean squared error using the mean label value for the predictions, equal to the label variance of the node\n",
    "* **value**: the mean label value of all observations in the node\n",
    "\n",
    "The variance, equal to the mean squared error, of a child node _may_ be greater than the parent node but the overall weighted variance as a result of the split is lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree training algorithm\n",
    "\n",
    "There are several algorithms available to determine node splits in decision trees. `scikit-learn` uses the   Classification And Regression Tree (CART) algorithm. The steps in the algorithm are\n",
    "\n",
    "1. For each feature $p$, construct _thresholds_ $t_{pi}$. An example of a threshold for a feature is petal width $\\leq$ 0.8 cm.\n",
    "1. Choose the threshold that results in the greatest reduction of the _weighted_ error metric. For example, using the criterion petal width $\\leq$ 0.8 cm leads to the greatest drop in overall Gini impurity from the resulting two new nodes.\n",
    "1. Split the data set into two sets, nodes, using the chosen threshold.\n",
    "1. Repeat the process on the _child_ nodes until a termination criterion is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The node splitting process is _greedy_, the model chooses the split point that will result in the greatest reduction of the loss metric at the moment. Greedy algorithms are those that make the locally optimal choice. They do not consider making suboptimal choices that may setup an overall better choice in later iterations. Greedy algorithms may not always converge to the global minimum but will often have better time complexities than algorithms that may result in a more optimal solution. The choice of a greedy algorithm for decision trees is an example of trading some model performance, ideally a small amount, for a gain in faster computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Hyperparameters\n",
    "\n",
    "As you have seen before, the `max_depth` is a key hyperparameter for decision trees. It controls how deep the tree is allowed to grow, thus, how adaptive the model is to fit the training data. As the depth gets greater, the model gets more complex, and has a higher propensity for large variance error. The `max_depth` isn't the only hyperparameter you can tune; several others are\n",
    "\n",
    "<table>\n",
    "\t<tr>\n",
    "    <th>Hyperparameter</th>\n",
    "    <th>Description</th>\n",
    "\t</tr>\n",
    "\n",
    "    <tr>\n",
    "    <td>`max_depth`</td>\n",
    "    <td>The maximum depth of the tree </td>\n",
    "\t</tr>\n",
    "\t\n",
    "    <tr>\n",
    "    <td>`max_features`</td>\n",
    "    <td>The number of features to consider when deciding the best split</td>\n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "    <td>`min_samples_split`</td>\n",
    "    <td>Minimum number of samples required to consider a split on an internal node</td>\n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "    <td>`min_samples_leaf`</td>\n",
    "    <td>Minimum number of samples required for a leaf (terminal node)</td>\n",
    "\t</tr>\n",
    "</table>\n",
    "\n",
    "As with any machine learning model, it is important to understand how the hyperparameters controls the model's bias and variance error. The best way to analyze a hyperparameter's affect on a model is to consider whether it increases or decreases the model's ability to adapt to the training data. As always, `scikit-learn` has extensive documentation that details all of the hyperparameters available and their meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "* For each hyperparameter, how does it affect a model's bias and variance error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric interpretation\n",
    "\n",
    "It sometimes helps to visualize how our model is making decision through the use of a decision boundary. We'll train a decision tree classifier using a data set with two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=250, noise=0.25, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "def tree_decision_boundary(max_depth=5, min_samples_leaf=2):\n",
    "    tree = DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
    "    tree.fit(X_train, y_train)\n",
    "    accuracy = tree.score(X_test, y_test)\n",
    "    \n",
    "    X1, X2 = np.meshgrid(np.linspace(-2, 3), np.linspace(-2, 2))\n",
    "    y_proba = tree.predict_proba(np.hstack((X1.reshape(-1, 1), X2.reshape(-1, 1))))[:, 1]\n",
    "    plt.contourf(X1, X2, y_proba.reshape(50, 50),  16, cmap=plt.cm.bwr, alpha=0.75)\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='white', cmap=plt.cm.bwr)\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.title('accuracy: {}'.format(accuracy));\n",
    "\n",
    "depth_slider = IntSlider(min=1, max=40, step=1, description='max depth')\n",
    "min_samples_leaf_slider = IntSlider(min=1, max=20, step=1, description='min leaf size')\n",
    "interact(tree_decision_boundary, max_depth=depth_slider, min_samples_leaf=min_samples_leaf_slider);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure, we see how the model is creating partitions in our feature space, attempting to box in the observations belonging to the same class. The partitions are perpendicular to an axis. Because of the perpendicular partitions, decision trees work best when the features are not strongly correlated. A technique like principal component analysis that generate linearly uncorrelated features help out decision tree models.\n",
    "\n",
    "**Question**\n",
    "* For decision trees, there is no need to scale your data. Why is this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and predicting time complexities\n",
    "\n",
    "An important consideration when choosing a machine learning model is the time complexity for training and making predictions. Consider a balanced tree where the number of nodes and branches at every level/depth of the tree is equal. At most, the depth of the tree will be $\\log(n)$ since the number of nodes is equal to $2^d$. For the CART algorithm, at each depth, we must evaluate $np$ possible cut points. Overall, the time complexity for training is $O(np\\log(n))$.\n",
    "\n",
    "Making a prediction involves traversing the tree from root to terminal node. Once again assuming a balanced tree, the model will make $\\log(n)$ decisions to arrive at a leaf. The predicting time complexity is simply $O(\\log(n))$. In general, making predictions using decision trees is faster than training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble models\n",
    "\n",
    "Ensemble models are machine learning models that use more than one predictor to arrive at a prediction. A group of predictors form an _ensemble_. In general, ensemble models perform better than using a single predictor. There are three types of ensemble models, bagging, boosting, and blending. Of the three, decision trees have been used to construct bagging and boosting based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forests\n",
    "\n",
    "The performance of a single decision tree will be limited. Instead of relying on one tree, a better approach is to aggregate the predictions of multiple tree. On average, aggregation will perform better than a single predictor. You can envision the aggregation as mimicking the idea of \"wisdom of the crowd\". We call a tree based model that aggregates the predictions of multiple trees a __random forest__.\n",
    "\n",
    "In order for a random forest to be effective, the model needs a diverse collection of trees. There should be variations in the chosen thresholds for splitting and the number of nodes and branches. There is no point in aggregating the predicted results if all the trees are nearly identical and produce the same result. There is no \"wisdom of the crowd\" if everyone thinks alike. To achieve a diverse set of trees, we need to:\n",
    "\n",
    "1. Train each tree in the forest using a different training set.\n",
    "1. Only consider a subset of features when deciding how to split the nodes.\n",
    "\n",
    "For the first point, ideally we would generate a new training set for each tree. However, often times it's too difficult or expensive to collect more data; we have to make due with what we have. Bootstrapping is a general statistical technique to generate \"new\" data sets with a single set by random sampling with _replacement_. Sampling with replacement allows for a data point to be sampled more than once.\n",
    "\n",
    "Typically, when training the standard decision tree model, the algorithm will consider all features in deciding the node split. Considering only a subset of your features ensures that your trees do not resemble each other. If the algorithm had considered all features, a dominant feature would be continuously chosen for node splits.\n",
    "\n",
    "The hyperparameters available for random forests include those of decision tress with some additions.\n",
    "\n",
    "<table>\n",
    "\t<tr>\n",
    "    <th>Hyperparameter</th>\n",
    "    <th>Description</th>\n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "    <td>`n_estimators`</td>\n",
    "    <td>The number of trees in the forest</td>\n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "    <td>`n_jobs`</td>\n",
    "    <td>The number of jobs to run in parallel when fitting and predicting</td>\n",
    "\t</tr>\n",
    "    \n",
    "    <tr>\n",
    "    <td>`warm_start`</td>\n",
    "    <td> If set to `True`, reuse the trained tree from a prior fitting and just train the additional trees\n",
    "    </td>\n",
    "\t</tr>\n",
    "</table>\n",
    "\n",
    "Since the random forest is based on idea of bootstrapping and aggregating the results, it is referred to as a *bagging* ensemble model.\n",
    "\n",
    "**Question**\n",
    "* Are there any other advantages of considering a subset of the features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct a plot of the test set error as a function of the size of the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=100, n_informative=20, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "def rf_mse(max_features='sqrt', n_max=50):\n",
    "    \"\"\"Generate mean squared errors for growing random forest.\"\"\"\n",
    "    \n",
    "    rgr = RandomForestRegressor(max_features=max_features,\n",
    "                                max_depth=8, n_estimators=1, \n",
    "                                warm_start=True, \n",
    "                                random_state=0)\n",
    "    mse = np.zeros(n_max)\n",
    "\n",
    "    for n in range(1, n_max):\n",
    "        rgr.set_params(n_estimators=n)\n",
    "        rgr.fit(X_train, y_train)\n",
    "        mse[n-1] = mean_squared_error(y_test, rgr.predict(X_test))\n",
    "\n",
    "    return mse\n",
    "\n",
    "for param in ('sqrt', 'log2'):\n",
    "    mse = rf_mse(max_features=param)\n",
    "    plt.plot(mse[:-1])\n",
    "\n",
    "plt.xlabel('number of trees')\n",
    "plt.ylabel('mean squared error')\n",
    "plt.legend(['sqrt', 'log2', 'all']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several takeaways from the results plotted above.\n",
    "\n",
    "1. There is sharp drop in error when initially growing the forest.\n",
    "1. In general, as the number trees increase, performance increases.\n",
    "1. You _can_ overfit with a large number of trees but the model is robust to overfitting with the number of trees\n",
    "1. Note, the model was not tuned for hyperparameters like `max_depth` and `min_samples_split`.\n",
    "\n",
    "The initial drop in error can be attributed to a large increase in diverse trees when the forest is small. In other words, the additional trees will be very different from the current trees simply because the forest is small. The increase in tree diversity drives predictive power. As the forest grows, newer trees will not be significantly different from the current pool of trees as bootstrapping is no longer producing substantially diverse training sets to create very different looking trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extremely Randomized Trees\n",
    "\n",
    "An extremely randomized trees is a variation of the random forest model that injects additional randomness. As with a random forest, a random subset of the features are selected to determine which one to use for a node split. However, instead of considering the optimal split point _for each_ selected feature, a candidate for the split for each feature is chosen at _random_. From these randomly chosen values, the best is chosen to perform the split. The extra randomness serves two folds: it helps reduce the model's variance and leads to faster training times. In `scikit-learn`, the extremely randomized tree model is provided by `ensemble.ExtraTreesClassifier` and `ensemble.ExtraTreesRegressor`.\n",
    "\n",
    "**Question**\n",
    "* Can you formulate the training time complexity for both random forests and extremely randomized trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Trees\n",
    "\n",
    "Gradient boosting trees are another ensemble model; it is collection of tree models arranged in a sequence. Here, the model is built stage-wise; each additional tree aims to correct the previously built model's predictions. A model with $M$ trees is equal to \n",
    "\n",
    "$$ f_M(x_j) =  \\sum^M_m \\gamma_m h_m(x_j), $$\n",
    "\n",
    "where $h_m$ is a **weak learner** decision tree, a \"stump\" model with low depth that performs poorly on its own. The term **boosting** refers to the algorithm's ability to combine multiple weak learners to form a strong learner. $\\gamma_m$ is a factor that scales the contribution of a tree to the overall model. How are $h_m$ and $\\gamma_m$ chosen? The model is usually initialized with $h_0$ being equal to the mean of the training labels for regression or the majority class for classification. We also need to choose a loss function $L(y, f_m)$. For example, if the loss function is squared error, then\n",
    "\n",
    "$$ L_{SE}(y_j, f_m(x_j)) = (y_j-f_m(x_j))^2. $$ \n",
    "\n",
    "The steps for building our model with $M$ trees at each stage is\n",
    "\n",
    "1. Compute the **pseudo-residuals**, the derivative of the loss function with respect to the previous model $f_{m-1}$. The equation for the pseudo-residuals for a model at stage $m$ is\n",
    "$$ r_{jm} = - \\left[\\frac{\\partial L(y_j, f(x_j))}{\\partial f} \\right]_{f(x_j) = f_{m-1}(x_j)}. $$\n",
    "1. Train $h_m$ on the **pseudo-residuals** $r_{jm}$.\n",
    "1. Choose $\\gamma_m$ that minimizes $L(y_j, f_{m-1}(x_j) + \\gamma_m h_m(x_j)).$\n",
    "1. Form the improved model equal to\n",
    "$$ f_m(x_j) = f_{m-1}(x_j) + \\gamma_m h_m(x_j). $$\n",
    "1. Repeat until the model includes all $M$ trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where does the name gradient come from in gradient boosting trees? Adding a model is analogous to a single iteration in gradient descent. Gradient descent is a minimization algorithm that updates/improves the current answer by taking a step in the direction of the negative of the gradient of the function that is being minimized. The pseudo-residuals represent the direction of greatest reduction in prediction error. Since $h_m$ is trained on the direction of greatest descent of the loss, it will be an approximation of the improvement required for our model to fit the data more closely. $\\gamma_m$ is the step size we should take in the direction of greatest model improvement. Compare the equation above for an improved model with the equation of gradient descent applied to a function $C(\\beta_i)$;\n",
    "\n",
    "$$\\beta^{updated}_i = \\beta^{current}_i - \\gamma \\left(\\left.\\frac{\\partial C}{\\partial \\beta_i}\\right)\\right|_{\\beta_i=\\beta^{current}_i}.$$\n",
    "\n",
    "The concept of pseudo-residuals allows us to generalize gradient boosting trees for any loss function. In fact, when we use a square loss function like the squared error, the pseudo-residual is directly proportional to the residual, $y_j - f(x_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting trees have a similar set of hyperparameters as random forests but with some key additions.\n",
    "\n",
    "<table>\n",
    "\t<tr>\n",
    "    <th>Hyperparameter</th>\n",
    "    <th>Description</th>\n",
    "\t</tr>\n",
    "    \n",
    "    <tr>\n",
    "    <td>`learning_rate`</td>\n",
    "    <td>Multiplicative factor of the tree's contribution to the model.</td>\n",
    "\t</tr>\n",
    "\n",
    "    <tr>\n",
    "    <td>`subsample`</td>\n",
    "    <td>Fraction of the training data to use when fitting the trees.</td>\n",
    "\t</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate ranges form 0 to 1, with the modified equation being\n",
    "\n",
    "$$ f_m(x_j) = f_{m-1}(x_j) + \\nu \\gamma_m h_m(x_j), $$\n",
    "\n",
    "where $\\nu$ is the learning rate. The learning rate and subsampling fraction interact with each other, we will see this in the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "def gb_mse(learning_rate=1.0, subsample=1.0, n_max=80):\n",
    "    \"\"\"Generate mean squared errors for growing gradient boosting trees.\"\"\"\n",
    "    \n",
    "    rgr = GradientBoostingRegressor(learning_rate=learning_rate,\n",
    "                                    subsample=subsample,\n",
    "                                    max_depth=2, \n",
    "                                    n_estimators=1, \n",
    "                                    warm_start=True, \n",
    "                                    random_state=0)\n",
    "    mse = np.zeros(n_max)\n",
    "\n",
    "    for n in range(1, n_max):\n",
    "        rgr.set_params(n_estimators=n)\n",
    "        rgr.fit(X_train, y_train)\n",
    "        mse[n-1] = mean_squared_error(y_test, rgr.predict(X_test))\n",
    "\n",
    "    return mse\n",
    "\n",
    "def gen_legend_str(hparams):\n",
    "    \"\"\"Generate strings for legend in plot.\"\"\"\n",
    "    \n",
    "    base_str = 'learning rate: {} subsample: {}'\n",
    "    \n",
    "    return [base_str.format(d['learning_rate'], d['subsample']) for d in hparams]\n",
    "\n",
    "hparams = ({'learning_rate': 1.0, 'subsample': 1.0},\n",
    "           {'learning_rate': 1.0, 'subsample': 0.5},\n",
    "           {'learning_rate': 0.75, 'subsample': 0.5},\n",
    "           {'learning_rate': 0.5, 'subsample': 0.5})\n",
    "\n",
    "for kwargs in hparams:\n",
    "    mse = gb_mse(**kwargs)\n",
    "    plt.plot(mse[:-1])\n",
    "\n",
    "legend_strs = gen_legend_str(hparams)\n",
    "plt.xlabel('number of trees')\n",
    "plt.ylabel('mean squared error')\n",
    "plt.legend(legend_strs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above has the same characteristics as that of random forests. In general:\n",
    "\n",
    "1. If you apply subsampling, you need to lower the learning rate.\n",
    "1. Decreasing the learning rate requires more trees.\n",
    "\n",
    "As with gradient descent, at some point, additional iterations result in negligible model improvement as the algorithm has converged.\n",
    "\n",
    "**Question**\n",
    "* If you have a model with high variance error, how should you modify the values of the subsampling fraction and learning rate to reduce variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "\n",
    "Decision trees have the capability to evaluate feature importance. The feature importance score is based on two metrics:\n",
    "\n",
    "* How many times a particularly feature was selected to split a node\n",
    "* The depth where the feature was selected to split the node\n",
    "\n",
    "If a feature is chosen numerous times to make a node split, then it is a feature that is useful in dividing/partitioning the training data. The second metric considers the impact of choosing a feature for a node split. For example, the feature chosen to perform the initial split of the entire data set will have a greater impact on the structure of the tree than node splits deeper in the tree. You can think of earlier decisions as working on higher level attributes while decisions deeper in the tree work to distinguish smaller scale features. The described metrics are combined with the decrease in error metric from a split to derived a normalized feature importance score. For ensemble models, the feature importance is averaged across all trees.\n",
    "\n",
    "As with other models, feature importance of a trained model is obtained with `feature_importances_` attribute. Let's derive the relative feature importances for the iris data set using a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# load data set\n",
    "data = load_iris()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "feature_names = data['feature_names']\n",
    "\n",
    "# tune random forest\n",
    "tree = RandomForestClassifier(n_estimators=20, random_state=0)\n",
    "param_grid = {'max_depth': range(2, 10), 'min_samples_split': [2, 4, 6, 8, 10]}\n",
    "grid_search = GridSearchCV(tree, param_grid, cv=3, n_jobs=2, verbose=1, iid=True)\n",
    "grid_search.fit(X, y)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# plot feature importance\n",
    "df = pd.DataFrame({'importance': best_model.feature_importances_}, index=feature_names)\n",
    "df.plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. For the California housing data set, tune `max_depth` and either `min_samples_split` or `min_samples_leaf` for a decision tree regressor.\n",
    "1. Repeat the previous task but use a random forest, extremely randomized trees, and a gradient boosting tree. Consider the available hyperparameters for each of the models and decide which ones to include when tuning. How do the  models perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
