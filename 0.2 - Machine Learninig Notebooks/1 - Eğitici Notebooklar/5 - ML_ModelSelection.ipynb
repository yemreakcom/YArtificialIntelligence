{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.datasets\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection\n",
    "<!-- requirement: images/ml_map.png -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined machine learning as the process of optimizing the cost function by tweaking the parameters of a model. We have defined a model as some map that uniquely maps input (features) to output (labels or target values). We've left this definition very vague, because there are many popular models used for machine learning. In this notebook we'll explore a few of them and investigate how to choose between different possible models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://dataincubator-wqu.s3.amazonaws.com/caldata/cal_housing.pkz -nc -P ~/scikit_learn_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "cali_data = fetch_california_housing()\n",
    "\n",
    "cali_df = pd.DataFrame(cali_data.data, columns=cali_data.feature_names)\n",
    "cali_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll again work with the California data set, predicting home values based on the features. While this is a regression task, the concepts involved in model selection also apply to classification tasks, and many models are capable of both regression and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis\n",
    "\n",
    "Model selection begins with characterizing our data: how many features are there, how many observations, are any features correlated, are the relationships between features and labels linear or nonlinear, is the variance in the features uniform, etc.? The `scikit-learn` documentation includes a [flowchart](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) illustrating just some possible models and factors we might consider while we're exploring our data set.\n",
    "\n",
    "![ml_model_flowchart](images/ml_map.png)\n",
    "\n",
    "We'll dive deeper into some of these considerations in future lessons. First let's visualize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature(feature):\n",
    "    plt.plot(cali_df[feature], cali_data.target, '.')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Median home value')\n",
    "\n",
    "menu = cali_data.feature_names\n",
    "\n",
    "interact(plot_feature, feature=menu);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between home value and some of the features looks very linear (e.g. median income), but others look non-linear or random (e.g. house age or geography). We've discussed [linear regression](ML_LinearRegression.ipynb) -- let's try modeling one of the non-linear features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "A decision tree is essentially a logic tree that branches based on feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from IPython import display\n",
    "from graphviz import Source\n",
    "\n",
    "model = tree.DecisionTreeRegressor(max_depth=2)\n",
    "model.fit(cali_df['Longitude'].to_frame(), cali_data.target)\n",
    "\n",
    "plt.plot(cali_df['Longitude'], cali_data.target, '.', label='data')\n",
    "plt.plot(cali_df['Longitude'].sort_values(), \n",
    "         model.predict(cali_df['Longitude'].sort_values().to_frame()), \n",
    "         'r-', label='model')\n",
    "\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Median home value')\n",
    "plt.legend()\n",
    "\n",
    "graph = Source(tree.export_graphviz(model, out_file=None, feature_names=['Longitude']))\n",
    "display.SVG(graph.pipe(format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By allowing for more branching, we can make our model more complex. Does this make our model better or worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "max_depths = range(1, 20)\n",
    "training_error = []\n",
    "for max_depth in max_depths:\n",
    "    model = tree.DecisionTreeRegressor(max_depth=max_depth)\n",
    "    model.fit(cali_df['Latitude'].to_frame(), cali_data.target)\n",
    "    training_error.append(mse(cali_data.target, model.predict(cali_df['Latitude'].to_frame())))\n",
    "\n",
    "plt.plot(max_depths, training_error)\n",
    "plt.xlabel('Maximum tree depth')\n",
    "plt.ylabel('Mean squared error');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reach a conflict: the model looks qualitatively worse beyond `max_depth > 5` but the error keeps dropping. This problem is called _overfitting_. The model looks worse because it doesn't follow the trend of the data, but instead follows the random noise. To detect overfitting, we need to see how our model generalizes to new data. We can do this artificially by withholding part of our data set during the training step, and then using it to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(*shuffle(cali_df['Longitude'].to_frame(), \n",
    "                                                             cali_data.target), test_size=0.1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_error = []\n",
    "for max_depth in max_depths:\n",
    "    model = tree.DecisionTreeRegressor(max_depth=max_depth)\n",
    "    model.fit(X_train, y_train)\n",
    "    testing_error.append(mse(y_test, model.predict(X_test)))\n",
    "\n",
    "plt.plot(max_depths, training_error, label='Training error')\n",
    "plt.plot(max_depths, testing_error, label='Testing error')\n",
    "plt.xlabel('Maximum tree depth')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing error confirms our suspicion. As the model becomes more complex, it improves up to a point, and then it loses generalizability. One way to track complexity is the number of _parameters_ in our model. If we labeled each layer of our tree (starting with 0), we see there are $2^i$ parameters on layer $i$. Therefore the total number of parameters is $\\sum_i 2**i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(max_depths, [sum([2**i for i in range(max_depth)]) for max_depth in max_depths]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have enough parameters, we can simply memorize our training set, by creating a complex logical tree that separates all the different $X$ values in our training into unique branches of our tree. But _memorization is not learning_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters: tuning and cross-validation\n",
    "\n",
    "When we defined our decision tree estimator, we chose how many layers the tree would have using the `max_depth` keyword. When we initiate an estimator, we can pass keyword arguments that will dictate its structure. The decision tree regressor accepts [12 different keyword arguments](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor). These arguments are called _hyperparameters_. This is in contrast to _parameters_, which are the the numbers that our model uses to predict labels based on features. _Parameters_ are optimized during training. _Hyperparameters_ are decided before training and dictate the model's structure. Basically all models have hyperparameters. Even a simple linear regressor has a hyperparameter `fit_intercept`.\n",
    "\n",
    "Since changing hyperparameters changes the structure of the model, we should think of choosing hyperparameters as part of model selection. `Scikit-learn` provides a useful tool for comparing different hyperparameter values, `GridSearchCV`. There are two ideas behind `GridSearchCV`: first we will split up the data into a training and validation set (using a method called [k-folds](http://scikit-learn.org/stable/modules/cross_validation.html#k-fold)) and then we train and evaluate models with different hyperparameters selected from a grid of combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = tree.DecisionTreeRegressor()\n",
    "\n",
    "gs = GridSearchCV(model,\n",
    "                  {'max_depth': range(1, 15),\n",
    "                  'min_samples_split': range(10, 110, 10)},\n",
    "                  cv=5,\n",
    "                  n_jobs=2,\n",
    "                  scoring='neg_mean_squared_error')\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gs.best_estimator_\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plt.plot(cali_df['Longitude'], cali_data.target, '.', label='data')\n",
    "plt.plot(cali_df['Longitude'].sort_values(), model.predict(cali_df['Longitude'].sort_values().to_frame()), 'r-', label='model')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Median home value')\n",
    "plt.legend()\n",
    "\n",
    "print(mse(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing different estimators\n",
    "\n",
    "`Scikit-learn`'s `GridSearchCV` only takes one estimator, so while it is useful for hyperparameter tuning, we can't use it to compare different estimators. However, we could easily iterate over different estimators, tune their hyperparameters using `GridSearchCV`, and use a metric to compare their performance.\n",
    "\n",
    "For example, K-Neighbors is a very flexible model that uses the k-nearest points in feature space to predict the value of a new observation. Will this perform better than our decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "tree_reg = tree.DecisionTreeRegressor()\n",
    "knn_reg = KNeighborsRegressor()\n",
    "\n",
    "estimators = {'tree_reg': tree_reg, 'knn_reg': knn_reg}\n",
    "hyperparam_dict = {'tree_reg': {'min_samples_split': range(10, 110, 10), 'max_depth': range(1, 15)}, 'knn_reg': {'n_neighbors': range(10, 100, 10)}}\n",
    "\n",
    "scores = {}\n",
    "for name, estimator in estimators.items():\n",
    "    gs = GridSearchCV(estimator,\n",
    "                      hyperparam_dict[name],\n",
    "                      cv=5,\n",
    "                      n_jobs=2,\n",
    "                      scoring='neg_mean_squared_error')\n",
    "    \n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    scores[name] = -gs.best_score_\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "- K-Neighbors performed better than our decision tree; why might we pick decision tree even though it performs worse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
