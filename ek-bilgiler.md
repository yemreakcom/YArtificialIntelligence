# â­ Ek Bilgiler

## â¬ Coursera Ã‡alÄ±ÅŸma DosyalarÄ±nÄ± Ä°ndirme

* Ã‡alÄ±ÅŸma alanÄ± sayfanÄ±za girin \(dosyalarÄ±n olduÄŸu yer\)
* Alttaki komutu kopyalayÄ±n
* Tekrar dosyalarÄ±n olduÄŸu yere gelin ve orada beliren `workspace.tar.gz` dosyasÄ±nÄ± indirin

```bash
cd ~/work
tar cvfzh ~/workspace.tar.gz *
mv ~/workspace.tar.gz ~/work/workspace.tar.gz
```

### **ğŸ³ DosyalarÄ±n Boyutu 200MB'den Fazla Ä°se**

* Alttaki komutu kopyalayÄ±n ve tek tek indirin
* Ä°ndirdikten sonra dosyalarÄ± `cat workspace.tar.gz.part.* > workspace.tar.gz` ile birleÅŸtirebilirsiniz

```bash
split -b 200m workspace.tar.gz workspace.tar.gz.part.
```

> [Downloading all the assignments jupyter notebooks and files](https://www.reddit.com/r/learnmachinelearning/comments/7er5ps/coursera_downloading_all_the_assignments_jupyter/)

## Uygulamalar

* [Java Neural Network Framework](http://neuroph.sourceforge.net/)

## Cloud Destekleri

[Buraya](https://github.com/discdiver/deep-learning-cloud-providers/blob/master/list.md) ve [buraya](https://towardsdatascience.com/maximize-your-gpu-dollars-a9133f4e546a) tÄ±klayarak makine Ã¶ÄŸrenimi iÃ§in bulut hizmetlerine bakabilirsin.

* [Paperspace](https://www.paperspace.com/ml)

## Derin Ã–ÄŸrenme NotlarÄ±

> Her zaman yapÄ±lacak iÅŸ: GiriÅŸler ile aÄŸÄ±rlÄ±klarÄ± Ã§arp, sabit deÄŸiÅŸken \(sapma\) ile topla ve aktivasyon uygula!

![Aktivat&#xF6;r Fonksiyonu](https://cdn-images-1.medium.com/max/600/1*FLoEcD4bWRw6Zno32uFwuw.png)

* Aktivasyon fonksiyonu kullanÄ±lmayan bir sinir aÄŸÄ± sÄ±nÄ±rlÄ± Ã¶ÄŸrenme gÃ¼cÃ¼ne sahip bir doÄŸrusal baÄŸlanÄ±m \(linear regression\) gibi davranacaktÄ±r.
* Birden fazla dereceye sahip olan fonksiyonlara doÄŸrusal olmayan fonksiyonlar deriz
* AÄŸÄ±rlÄ±klar ile ilgili hata deÄŸerlerini hesaplamak iÃ§in yapay sinir aÄŸÄ±nda hatanÄ±n geriye yayÄ±lÄ±mÄ± algoritmasÄ± uygulanmaktadÄ±r. \(Backward Propagation\)

### Aktivasyon FonksiyonlarÄ±

> Aktivasyon fonksiyonu tÃ¼m bu bilgiler ve sizin yapay Ã¶ÄŸrenme modelinizin gereksinimlerine gÃ¶re karar vermeniz gereken kritik bir optimizasyon problemidir.

* Sigmoid Fonksiyonu
* ReLU \(Rectified Linear Unit\) Fonksiyonu
  * Derin Ã¶ÄŸrenme modelleri denemelere bu fonksiyon ile baÅŸlanmasÄ± tavsiye edilir.
  * HÄ±z bakÄ±mÄ±ndan avantajlÄ±dÄ±r. GradyanlarÄ±n Ã¶lmesi gibi bir problemi vardÄ±r.
  * Genellikle Ã§Ä±kÄ±ÅŸ deÄŸil ara katmanlarda kullanÄ±lÄ±r.
* Softmax Fonksiyonu
  * Genelde Ã§Ä±kÄ±ÅŸ iÃ§in kullanÄ±lÄ±r.
* Basamak \(Step\) Fonksiyonu
* DoÄŸrusal \(Linear\) Fonksiyon
* Hiperbolik Tanjant Fonksiyonu
* SÄ±zÄ±ntÄ± \(Leaky\) ReLU Fonksiyonu
* Swish \(A Self-Gated/Kendinden GeÃ§itli\) Fonksiyonu

### Aktivasyon FonksiyonlarÄ±nÄ±n Ã–zellikleri

> \(0, 1\) arasÄ±dan olan fonksyionlar \*_olasÄ±lÄ±ksal_- fonksiyonlardÄ±r.

![Aktivasyon Fonkisyonlar&#x131;n&#x131;n &#xD6;zellikleri](https://cdn-images-1.medium.com/max/800/1*lI22JpQMrlx777AOhzvjcw.png)

## Yapay Zeka KullanÄ±m AlanlarÄ±

* Otamatik Al-Sat iÅŸlemleri ile kullanÄ±lÄ±r.
* [El ile emoji oluÅŸturma](https://www.linkedin.com/feed/update/urn:li:ugcPost:6531200017103880192)

## Harici BaÄŸlantÄ±lar

### AraÅŸtÄ±rmacÄ±lar Web Siteleri

* [NVIDIA AI](https://www.nvidia.com/en-us/research/ai-playground/#)

### GitHub KaynaklarÄ±

#### Firmalar

* [NVIDIA](https://github.com/NVIDIA)

#### Karma

* [DeepLeague](https://github.com/farzaa/DeepLeague)

### Derin Ã–ÄŸrenme KaynaklarÄ±

* [Derin Ã–ÄŸrenme Ä°Ã§in Aktivasyon FonksiyonlarÄ±nÄ±n KarÅŸÄ±laÅŸtÄ±rÄ±lmasÄ±](https://medium.com/deep-learning-turkiye/derin-%C3%B6%C4%9Frenme-i%C3%A7in-aktivasyon-fonksiyonlar%C4%B1n%C4%B1n-kar%C5%9F%C4%B1la%C5%9Ft%C4%B1r%C4%B1lmas%C4%B1-cee17fd1d9cd)

### Motivasyon

* [Yapay zeka GauGAN, Ã§izimleri tabloya dÃ¶nÃ¼ÅŸtÃ¼rmeyi Ã¶ÄŸrendi!](https://www.youtube.com/watch?v=1iMmenHFdCE)
* [AYNA DÃœNYALAR](https://www.youtube.com/watch?v=-3DvuLtuf1U)

